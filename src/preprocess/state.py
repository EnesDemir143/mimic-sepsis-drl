"""
MIMIC-IV Sepsis DRL â€” 48-Feature State Vector Builder
======================================================
Mevcut ``mimic_hourly_binned.parquet`` ve raw MIMIC-IV dosyalarÄ±ndan
48 klinik Ã¶zelliÄŸi Ã§Ä±kararak ``data/processed/state.parquet`` Ã¼retir.

KullanÄ±m
--------
    uv run python -m src.preprocess
"""
from __future__ import annotations

import polars as pl
from tqdm import tqdm

from src.preprocess.config import (
    ADMISSIONS_PATH,
    CHARTEVENTS_PATH,
    DIAGNOSES_ICD_PATH,
    ELIXHAUSER_ICD9,
    ELIXHAUSER_ICD10,
    EXTRA_LAB_ITEMS,
    HOURLY_BINNED_PATH,
    ICUSTAYS_PATH,
    LABEVENTS_PATH,
    META_COLUMNS,
    OMR_PATH,
    STATE_FEATURES,
    STATE_PARQUET_PATH,
    VASOPRESSOR_CONVERSION,
    WEIGHT_CHART_ITEMIDS,
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Mevcut Hourly-Binned Parquet YÃ¼kleme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_hourly_binned() -> pl.DataFrame:
    """``mimic_hourly_binned.parquet`` dosyasÄ±nÄ± yÃ¼kler."""
    print("ðŸ“‚ Hourly-binned parquet yÃ¼kleniyor â€¦")
    df = pl.read_parquet(HOURLY_BINNED_PATH)
    print(f"   âœ… {df.shape[0]:,} satÄ±r, {df.shape[1]} sÃ¼tun yÃ¼klendi.")
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Eksik Lab Parametrelerini Ã‡ekme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def extract_extra_labs(stay_ids: set[int], icustay_map: pl.DataFrame) -> pl.DataFrame:
    """
    ``labevents.csv.gz`` dosyasÄ±ndan 10 eksik lab parametresini Ã§eker,
    saatlik bin'lere ortalamasÄ±nÄ± alÄ±r ve (stay_id, hour_bin, feature)
    formatÄ±nda dÃ¶ndÃ¼rÃ¼r.

    Parameters
    ----------
    stay_ids : set[int]
        Ä°lgilenilen stay_id kÃ¼mesi.
    icustay_map : pl.DataFrame
        ``stay_id â†’ (subject_id, hadm_id, intime, outtime)`` eÅŸlemesi.
    """
    print("ðŸ§ª Eksik lab parametreleri labevents'ten Ã§ekiliyor â€¦")

    # Hedef item ID'lerini dÃ¼zle
    all_item_ids: list[int] = []
    item_to_name: dict[int, str] = {}
    for name, ids in EXTRA_LAB_ITEMS.items():
        for iid in ids:
            all_item_ids.append(iid)
            item_to_name[iid] = name

    # labevents lazy scan â€” sadece gerekli sÃ¼tunlar
    labs_raw = (
        pl.scan_csv(
            LABEVENTS_PATH,
            schema_overrides={
                "hadm_id": pl.Utf8,      # null olabiliyor
                "valuenum": pl.Float64,
                "itemid": pl.Int64,
                "subject_id": pl.Int64,
            },
            infer_schema_length=10000,
        )
        .select(["subject_id", "hadm_id", "itemid", "charttime", "valuenum"])
        .filter(pl.col("itemid").is_in(all_item_ids))
        .filter(pl.col("valuenum").is_not_null())
        .with_columns(
            pl.col("hadm_id").cast(pl.Int64, strict=False),
            pl.col("charttime").str.to_datetime("%Y-%m-%d %H:%M:%S", strict=False),
        )
        .collect()
    )
    print(f"   ðŸ“Š {labs_raw.shape[0]:,} lab Ã¶lÃ§Ã¼mÃ¼ Ã§ekildi.")

    # ICU stay sÃ¼relerine gÃ¶re filtrele
    labs_with_stay = (
        labs_raw
        .join(icustay_map, on=["subject_id", "hadm_id"], how="inner")
        .filter(
            (pl.col("charttime") >= pl.col("intime"))
            & (pl.col("charttime") <= pl.col("outtime"))
        )
    )

    # Feature ismi ekle
    labs_with_stay = labs_with_stay.with_columns(
        pl.col("itemid").replace_strict(item_to_name, default=None).alias("feature_name")
    )

    # Saatlik bin'e yuvarlama
    labs_with_stay = labs_with_stay.with_columns(
        pl.col("charttime").dt.truncate("1h").alias("hour_bin")
    )

    # Saatlik ortalama â†’ pivot
    labs_hourly = (
        labs_with_stay
        .group_by(["stay_id", "hour_bin", "feature_name"])
        .agg(pl.col("valuenum").mean().alias("value"))
        .pivot(on="feature_name", index=["stay_id", "hour_bin"], values="value")
    )

    print(f"   âœ… {labs_hourly.shape[0]:,} satÄ±r lab verisi pivot edildi.")
    return labs_hourly


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. AÄŸÄ±rlÄ±k Ã‡ekme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def extract_weight(icustay_map: pl.DataFrame) -> pl.DataFrame:
    """
    Hasta baÅŸÄ±na aÄŸÄ±rlÄ±k (kg) Ã§eker.
    Ã–nce chartevents'ten Admission Weight â†’ yoksa OMR'den Weight (Lbs) â†’ kg.
    SonuÃ§: ``stay_id â†’ weight_kg`` (tek deÄŸer / hasta).
    """
    print("âš–ï¸  AÄŸÄ±rlÄ±k bilgisi Ã§ekiliyor â€¦")

    # 1) chartevents'ten
    weight_chart = (
        pl.scan_csv(
            CHARTEVENTS_PATH,
            infer_schema_length=10000,
            schema_overrides={"valuenum": pl.Float64, "itemid": pl.Int64,
                              "stay_id": pl.Int64},
        )
        .select(["stay_id", "itemid", "valuenum"])
        .filter(
            pl.col("itemid").is_in(WEIGHT_CHART_ITEMIDS)
            & pl.col("valuenum").is_not_null()
            & (pl.col("valuenum") > 10)     # saÃ§ma deÄŸerleri ele
            & (pl.col("valuenum") < 300)
        )
        .group_by("stay_id")
        .agg(pl.col("valuenum").first().alias("weight_kg"))
        .collect()
    )
    print(f"   ðŸ“Š chartevents'ten {weight_chart.shape[0]:,} hasta aÄŸÄ±rlÄ±ÄŸÄ± bulundu.")

    # 2) OMR'den eksik kalan hastalar iÃ§in
    found_stays = set(weight_chart["stay_id"].to_list())
    missing_subject_ids = (
        icustay_map
        .filter(~pl.col("stay_id").is_in(found_stays))
        .select("subject_id")
        .unique()
    )

    if missing_subject_ids.shape[0] > 0:
        omr = pl.read_csv(OMR_PATH)
        omr_weight = (
            omr
            .filter(pl.col("result_name").str.contains("Weight"))
            .filter(pl.col("result_name").str.contains("Lbs"))
            .with_columns(
                pl.col("result_value")
                .cast(pl.Float64, strict=False)
                .alias("weight_lbs")
            )
            .filter(pl.col("weight_lbs").is_not_null())
            .group_by("subject_id")
            .agg((pl.col("weight_lbs").first() * 0.453592).alias("weight_kg"))
        )

        # subject_id â†’ stay_id mapping
        omr_with_stay = (
            icustay_map
            .filter(~pl.col("stay_id").is_in(found_stays))
            .select(["stay_id", "subject_id"])
            .join(omr_weight, on="subject_id", how="inner")
            .select(["stay_id", "weight_kg"])
        )

        weight_chart = pl.concat([weight_chart, omr_with_stay])
        print(f"   ðŸ“Š OMR'den {omr_with_stay.shape[0]:,} ek aÄŸÄ±rlÄ±k eklendi.")

    print(f"   âœ… Toplam {weight_chart.shape[0]:,} hasta iÃ§in aÄŸÄ±rlÄ±k mevcut.")
    return weight_chart


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Elixhauser Komorbidite Skoru
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def compute_elixhauser(icustay_map: pl.DataFrame) -> pl.DataFrame:
    """
    ``diagnoses_icd.csv.gz``'den ICD-9/10 kodlarÄ±nÄ± okuyarak
    hasta baÅŸÄ±na Elixhauser komorbidite sayÄ±sÄ± hesaplar.
    SonuÃ§: ``stay_id â†’ elixhauser_score``.
    """
    print("ðŸ¥ Elixhauser komorbidite skoru hesaplanÄ±yor â€¦")

    diag = pl.read_csv(
        DIAGNOSES_ICD_PATH,
        schema_overrides={"icd_code": pl.Utf8, "icd_version": pl.Int64},
    )
    diag = diag.with_columns(pl.col("icd_code").cast(pl.Utf8).str.strip_chars())

    # hadm_id â†’ stay_id mapping
    stay_hadm = icustay_map.select(["stay_id", "hadm_id"]).unique()
    diag_with_stay = diag.join(stay_hadm, on="hadm_id", how="inner")

    # Her ICD kodu iÃ§in Elixhauser kategorilerini eÅŸle
    def _match_categories(row_icd_code: str, row_icd_version: int) -> set[str]:
        """Tek bir ICD kodunun hangi Elixhauser kategorilerine dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ dÃ¶ndÃ¼rÃ¼r."""
        mapping = ELIXHAUSER_ICD9 if row_icd_version == 9 else ELIXHAUSER_ICD10
        matched: set[str] = set()
        code_str = str(row_icd_code).strip()
        for cat, prefixes in mapping.items():
            for prefix in prefixes:
                if code_str.startswith(prefix):
                    matched.add(cat)
                    break
        return matched

    # Polars UDF yerine Python-level hesaplama (diagnoses genelde kÃ¼Ã§Ã¼k)
    records: dict[int, set[str]] = {}
    for row in diag_with_stay.iter_rows(named=True):
        sid = row["stay_id"]
        cats = _match_categories(row["icd_code"], row["icd_version"])
        if sid not in records:
            records[sid] = set()
        records[sid].update(cats)

    elix_data = [
        {"stay_id": sid, "elixhauser_score": len(cats)}
        for sid, cats in records.items()
    ]

    if not elix_data:
        # HiÃ§ eÅŸleÅŸme yoksa boÅŸ DF
        return pl.DataFrame({"stay_id": pl.Series([], dtype=pl.Int64),
                             "elixhauser_score": pl.Series([], dtype=pl.Int32)})

    result = pl.DataFrame(elix_data).with_columns(
        pl.col("elixhauser_score").cast(pl.Int32)
    )
    print(f"   âœ… {result.shape[0]:,} hasta iÃ§in Elixhauser skoru hesaplandÄ±.")
    print(f"   ðŸ“Š Skor daÄŸÄ±lÄ±mÄ±: min={result['elixhauser_score'].min()}, "
          f"max={result['elixhauser_score'].max()}, "
          f"mean={result['elixhauser_score'].mean():.1f}")
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ICU Readmission Flag
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def compute_icu_readmission(icustay_map: pl.DataFrame) -> pl.DataFrame:
    """
    AynÄ± ``hadm_id`` altÄ±nda birden fazla ICU stay varsa
    ilki hariÃ§ diÄŸerleri readmission=1 olarak iÅŸaretlenir.
    SonuÃ§: ``stay_id â†’ icu_readmission`` (0/1).
    """
    print("ðŸ”„ ICU readmission flag hesaplanÄ±yor â€¦")

    ordered = icustay_map.sort(["hadm_id", "intime"])

    readmit = (
        ordered
        .with_columns(
            pl.col("stay_id")
            .cum_count()
            .over("hadm_id")
            .alias("_seq")
        )
        .with_columns(
            pl.when(pl.col("_seq") > 1)
            .then(1)
            .otherwise(0)
            .cast(pl.Int32)
            .alias("icu_readmission")
        )
        .select(["stay_id", "icu_readmission"])
    )

    n_readmit = readmit.filter(pl.col("icu_readmission") == 1).shape[0]
    print(f"   âœ… {n_readmit:,} / {readmit.shape[0]:,} stay readmission olarak iÅŸaretlendi.")
    return readmit


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. TÃ¼retilen Klinik Ã–zellikler
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def compute_derived_features(df: pl.DataFrame) -> pl.DataFrame:
    """
    Mevcut sÃ¼tunlardan tÃ¼retilen Ã¶zellikleri hesaplar:
    - Total Vasopressor Equivalent
    - SOFA Score (6 organ)
    - SIRS Score
    - Shock Index
    - PaO2/FiO2 Ratio
    - Mechanical Ventilation flag
    - Cumulative Fluid Balance
    - HCO3 (= bicarbonate alias)
    """
    print("ðŸ”¬ TÃ¼retilen klinik Ã¶zellikler hesaplanÄ±yor â€¦")

    # â”€â”€ Total Vasopressor Equivalent â”€â”€
    vaso_cols = list(VASOPRESSOR_CONVERSION.keys())
    df = df.with_columns(
        [pl.col(c).fill_null(0.0) for c in vaso_cols if c in df.columns]
    )
    vaso_expr = [
        pl.col(col) * rate
        for col, rate in VASOPRESSOR_CONVERSION.items()
        if col in df.columns
    ]
    df = df.with_columns(
        pl.sum_horizontal(vaso_expr).alias("total_vaso_equiv")
    )

    # â”€â”€ SOFA Score â”€â”€
    df = _compute_sofa(df)

    # â”€â”€ SIRS Score â”€â”€
    df = _compute_sirs(df)

    # â”€â”€ Shock Index â”€â”€
    df = df.with_columns(
        pl.when(pl.col("sbp") > 0)
        .then(pl.col("heart_rate") / pl.col("sbp"))
        .otherwise(None)
        .alias("shock_index")
    )

    # â”€â”€ PaO2/FiO2 OranÄ± â”€â”€
    df = df.with_columns(
        pl.when(
            pl.col("fio2").is_not_null() & (pl.col("fio2") > 0)
            & pl.col("pao2").is_not_null()
        )
        .then(
            pl.col("pao2") / pl.when(pl.col("fio2") > 1)
            .then(pl.col("fio2") / 100.0)
            .otherwise(pl.col("fio2"))
        )
        .otherwise(None)
        .alias("pf_ratio")
    )

    # â”€â”€ Mechanical Ventilation â”€â”€
    df = df.with_columns(
        pl.when(pl.col("fio2").is_not_null() & (pl.col("fio2") > 21))
        .then(1)
        .otherwise(0)
        .cast(pl.Int32)
        .alias("mechanical_ventilation")
    )

    # â”€â”€ Cumulative Fluid Balance â”€â”€
    df = df.with_columns(
        pl.col("crystalloid_ml").fill_null(0.0).alias("_fluid_in"),
        pl.col("urine_output").fill_null(0.0).alias("_fluid_out"),
    )
    df = df.sort(["stay_id", "hour_bin"])
    df = df.with_columns(
        (pl.col("_fluid_in") - pl.col("_fluid_out"))
        .cum_sum()
        .over("stay_id")
        .alias("cumulative_fluid_balance")
    )
    df = df.drop(["_fluid_in", "_fluid_out"])

    # â”€â”€ HCO3 (bicarbonate alias) â”€â”€
    df = df.with_columns(
        pl.col("bicarbonate").alias("hco3")
    )

    print("   âœ… TÃ¼m tÃ¼retilen Ã¶zellikler hesaplandÄ±.")
    return df


def _compute_sofa(df: pl.DataFrame) -> pl.DataFrame:
    """SOFA skoru: 6 organ sistemi (0â€“24)."""

    # Ã–nceden fio2_ratio hesapla (% ise /100)
    fio2_ratio = (
        pl.when(pl.col("fio2") > 1)
        .then(pl.col("fio2") / 100.0)
        .otherwise(pl.col("fio2"))
    )
    pf = pl.col("pao2") / fio2_ratio

    # Mekanik ventilasyon tahmini
    is_mv = pl.col("fio2").is_not_null() & (pl.col("fio2") > 21)

    # 1. Respiratory
    sofa_resp = (
        pl.when(pf.is_null() | pl.col("pao2").is_null() | pl.col("fio2").is_null())
        .then(0)
        .when((pf <= 100) & is_mv).then(4)
        .when((pf <= 200) & is_mv).then(3)
        .when(pf <= 200).then(2)
        .when(pf <= 300).then(1)
        .when(pf <= 400).then(1)
        .otherwise(0)
    )

    # 2. Cardiovascular (MAP + vasopressor)
    sofa_cardio = (
        pl.when(pl.col("total_vaso_equiv") > 0.5).then(4)
        .when(pl.col("total_vaso_equiv") > 0.1).then(3)
        .when(pl.col("total_vaso_equiv") > 0).then(2)
        .when(pl.col("mbp").is_not_null() & (pl.col("mbp") < 70)).then(1)
        .otherwise(0)
    )

    # 3. Renal (Creatinine)
    sofa_renal = (
        pl.when(pl.col("creatinine").is_null()).then(0)
        .when(pl.col("creatinine") >= 5.0).then(4)
        .when(pl.col("creatinine") >= 3.5).then(3)
        .when(pl.col("creatinine") >= 2.0).then(2)
        .when(pl.col("creatinine") >= 1.2).then(1)
        .otherwise(0)
    )

    # 4. Neurological (GCS)
    sofa_neuro = (
        pl.when(pl.col("gcs_total").is_null()).then(0)
        .when(pl.col("gcs_total") < 6).then(4)
        .when(pl.col("gcs_total") <= 9).then(3)
        .when(pl.col("gcs_total") <= 12).then(2)
        .when(pl.col("gcs_total") <= 14).then(1)
        .otherwise(0)
    )

    # 5. Coagulation (Platelets)
    sofa_coag = (
        pl.when(pl.col("platelet").is_null()).then(0)
        .when(pl.col("platelet") <= 20).then(4)
        .when(pl.col("platelet") <= 50).then(3)
        .when(pl.col("platelet") <= 100).then(2)
        .when(pl.col("platelet") <= 150).then(1)
        .otherwise(0)
    )

    # 6. Liver (Bilirubin)
    sofa_liver = (
        pl.when(pl.col("bilirubin_total").is_null()).then(0)
        .when(pl.col("bilirubin_total") >= 12.0).then(4)
        .when(pl.col("bilirubin_total") >= 6.0).then(3)
        .when(pl.col("bilirubin_total") >= 2.0).then(2)
        .when(pl.col("bilirubin_total") >= 1.2).then(1)
        .otherwise(0)
    )

    df = df.with_columns(
        (sofa_resp + sofa_cardio + sofa_renal + sofa_neuro + sofa_coag + sofa_liver)
        .cast(pl.Int32)
        .alias("sofa_score")
    )
    return df


def _compute_sirs(df: pl.DataFrame) -> pl.DataFrame:
    """SIRS skoru: 4 kriter (0â€“4)."""

    sirs_temp = (
        pl.when(pl.col("temp_c").is_null()).then(0)
        .when((pl.col("temp_c") > 38.0) | (pl.col("temp_c") < 36.0)).then(1)
        .otherwise(0)
    )

    sirs_hr = (
        pl.when(pl.col("heart_rate").is_null()).then(0)
        .when(pl.col("heart_rate") > 90).then(1)
        .otherwise(0)
    )

    sirs_rr = (
        pl.when(pl.col("resp_rate").is_null()).then(0)
        .when(pl.col("resp_rate") > 20).then(1)
        .otherwise(0)
    )

    sirs_wbc = (
        pl.when(pl.col("wbc").is_null()).then(0)
        .when((pl.col("wbc") > 12.0) | (pl.col("wbc") < 4.0)).then(1)
        .otherwise(0)
    )

    df = df.with_columns(
        (sirs_temp + sirs_hr + sirs_rr + sirs_wbc)
        .cast(pl.Int32)
        .alias("sirs_score")
    )
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. Imputation (LOCF + Median)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def apply_imputation(df: pl.DataFrame, feature_cols: list[str]) -> pl.DataFrame:
    """
    1. LOCF (Last Observation Carried Forward) â€” ``stay_id`` bazÄ±nda forward-fill.
    2. Kalan null'lar â†’ kolon medyanÄ± ile doldurma.
    """
    print("ðŸ©¹ Imputation uygulanÄ±yor (LOCF + median) â€¦")

    numeric_features = [
        c for c in feature_cols
        if c in df.columns and df[c].dtype in (pl.Float64, pl.Float32, pl.Int32, pl.Int64)
    ]

    # LOCF â€” per stay_id
    df = df.sort(["stay_id", "hour_bin"])
    df = df.with_columns(
        [pl.col(c).forward_fill().over("stay_id").alias(c) for c in numeric_features]
    )

    # Median fill â€” kalan null'lar
    medians = {c: df[c].median() for c in numeric_features}
    df = df.with_columns(
        [pl.col(c).fill_null(medians[c] if medians[c] is not None else 0)
         for c in numeric_features]
    )

    null_counts = {c: df[c].null_count() for c in numeric_features}
    still_null = {k: v for k, v in null_counts.items() if v > 0}
    if still_null:
        print(f"   âš ï¸  HÃ¢lÃ¢ null olan sÃ¼tunlar: {still_null}")
    else:
        print("   âœ… TÃ¼m null'lar dolduruldu.")

    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8. ICU Stay Mapping YÃ¼kleme
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_icustay_map() -> pl.DataFrame:
    """``icustays.csv.gz`` dosyasÄ±nÄ± yÃ¼kler ve gerekli sÃ¼tunlarÄ± dÃ¶ndÃ¼rÃ¼r."""
    print("ðŸ¥ ICU stay mapping yÃ¼kleniyor â€¦")
    icu = pl.read_csv(ICUSTAYS_PATH)
    icu = icu.with_columns(
        pl.col("intime").str.to_datetime("%Y-%m-%d %H:%M:%S", strict=False),
        pl.col("outtime").str.to_datetime("%Y-%m-%d %H:%M:%S", strict=False),
    )
    print(f"   âœ… {icu.shape[0]:,} ICU stay yÃ¼klendi.")
    return icu


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9. Ana Orchestrator
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run() -> None:
    """48-feature state vektÃ¶rÃ¼ pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r."""
    print("=" * 60)
    print("  MIMIC-IV Sepsis DRL â€” 48-Feature State Builder")
    print("=" * 60)

    # 1) Mevcut veriyi yÃ¼kle
    df = load_hourly_binned()
    stay_ids = set(df["stay_id"].unique().to_list())

    # 2) ICU stay mapping
    icustay_map = load_icustay_map()
    # Sadece bizim stay_id'lerimizi tut
    icustay_map = icustay_map.filter(pl.col("stay_id").is_in(stay_ids))

    # 3) Eksik lab parametreleri
    extra_labs = extract_extra_labs(stay_ids, icustay_map)
    df = df.join(extra_labs, on=["stay_id", "hour_bin"], how="left")
    print(f"   ðŸ“Š Lab merge sonrasÄ±: {df.shape[1]} sÃ¼tun")

    # 4) AÄŸÄ±rlÄ±k
    weight_df = extract_weight(icustay_map)
    df = df.join(weight_df, on="stay_id", how="left")

    # 5) Elixhauser skoru
    elix_df = compute_elixhauser(icustay_map)
    df = df.join(elix_df, on="stay_id", how="left")
    df = df.with_columns(pl.col("elixhauser_score").fill_null(0))

    # 6) ICU readmission
    readmit_df = compute_icu_readmission(icustay_map)
    df = df.join(readmit_df, on="stay_id", how="left")
    df = df.with_columns(pl.col("icu_readmission").fill_null(0))

    # 7) Gender encoding: M=0, F=1
    df = df.with_columns(
        pl.when(pl.col("gender") == "F")
        .then(1)
        .otherwise(0)
        .cast(pl.Int32)
        .alias("gender")
    )

    # 8) TÃ¼retilen Ã¶zellikler
    df = compute_derived_features(df)

    # 9) Imputation
    df = apply_imputation(df, STATE_FEATURES)

    # 10) Final sÃ¼tun seÃ§imi
    available = set(df.columns)
    missing_cols = [c for c in STATE_FEATURES if c not in available]
    if missing_cols:
        print(f"   âš ï¸  Eksik sÃ¼tunlar (0 ile doldurulacak): {missing_cols}")
        for c in missing_cols:
            df = df.with_columns(pl.lit(0.0).alias(c))

    final_cols = META_COLUMNS + STATE_FEATURES
    df = df.select(final_cols)

    # 11) Kaydet
    print(f"\nðŸ’¾ state.parquet kaydediliyor â†’ {STATE_PARQUET_PATH}")
    df.write_parquet(STATE_PARQUET_PATH)

    # 12) Ã–zet
    print("\n" + "=" * 60)
    print("  âœ… Pipeline tamamlandÄ±!")
    print(f"  ðŸ“Š Shape: {df.shape[0]:,} satÄ±r Ã— {df.shape[1]} sÃ¼tun")
    print(f"  ðŸ“‚ Ã‡Ä±ktÄ±: {STATE_PARQUET_PATH}")
    print("=" * 60)

    # Null Ã¶zet
    null_summary = df.null_count()
    print("\nðŸ“‹ Null sayÄ±larÄ±:")
    for col in STATE_FEATURES:
        nc = null_summary[col][0]
        if nc > 0:
            pct = nc / df.shape[0] * 100
            print(f"   {col}: {nc:,} ({pct:.1f}%)")

    print("\nðŸŽ¯ Ä°lk 3 satÄ±r:")
    print(df.head(3))
